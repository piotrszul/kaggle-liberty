
R version 3.1.3 (2015-03-09) -- "Smooth Sidewalk"
Copyright (C) 2015 The R Foundation for Statistical Computing
Platform: x86_64-unknown-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> library(caret)
Loading required package: lattice
Loading required package: ggplot2
> library(caretEnsemble)
> library(lattice)
> library(mlbench)
> library(doParallel)
Loading required package: foreach
Loading required package: iterators
Loading required package: parallel
> 
> getModelName<-function(modelSpec) {
+     paste(unlist(modelSpec), sep='_', collapse="_")
+ }
> 
> #"NormalizedGini" is the other half of the metric. This function does most of the work, though
> SumModelGini <- function(solution, submission) {
+     df = data.frame(solution = solution, submission = submission)
+     df <- df[order(df$submission, decreasing = TRUE),]
+     df
+     df$random = (1:nrow(df))/nrow(df)
+     df
+     totalPos <- sum(df$solution)
+     df$cumPosFound <- cumsum(df$solution) # this will store the cumulative number of positive examples found (used for computing "Model Lorentz")
+     df$Lorentz <- df$cumPosFound / totalPos # this will store the cumulative proportion of positive examples found ("Model Lorentz")
+     df$Gini <- df$Lorentz - df$random # will store Lorentz minus random
+     #print(df)
+     return(sum(df$Gini))
+ }
> 
> NormalizedGini <- function(solution, submission) {
+     SumModelGini(solution, submission) / SumModelGini(solution, solution)
+ }
> 
> greedOptGini <- function(X, Y, iter = 100L){
+     
+     N           <- ncol(X)
+     weights     <- rep(0L, N)
+     pred        <- 0 * X
+     sum.weights <- 0L
+     
+     while(sum.weights < iter) {
+         
+         sum.weights   <- sum.weights + 1L
+         pred          <- (pred + X) * (1L / sum.weights)
+         errors        <- apply(pred, MARGIN=2,function(x) {NormalizedGini(Y,x)})
+         best          <- which.max(errors)
+         weights[best] <- weights[best] + 1L
+         pred          <- pred[, best] * sum.weights
+     }
+     return(weights)
+ }
> 
> printf <- function(...) invisible(print(sprintf(...)))
> 
> train_data<- read.csv('data/train.csv', header=TRUE)
> test_data <- read.csv('data/test.csv', header=TRUE)
> #train_set <- subset(train_data,select=-c(Id,T1_V4,T1_V5,T1_V15,T1_V16, T1_V11))
> trainIdx <-createDataPartition(train_data$Hazard, p=0.01, list=FALSE)
> 
> train_set <- subset(train_data,select=-c(Id))
> 
> set.seed(37)
> 
> giniSummary <-function (data, lev = NULL, model = NULL) {
+ 
+     SumModelGini <- function(solution, submission) {
+         df = data.frame(solution = solution, submission = submission)
+         df <- df[order(df$submission, decreasing = TRUE),]
+         df
+         df$random = (1:nrow(df))/nrow(df)
+         df
+         totalPos <- sum(df$solution)
+         df$cumPosFound <- cumsum(df$solution) # this will store the cumulative number of positive examples found (used for computing "Model Lorentz")
+         df$Lorentz <- df$cumPosFound / totalPos # this will store the cumulative proportion of positive examples found ("Model Lorentz")
+         df$Gini <- df$Lorentz - df$random # will store Lorentz minus random
+         #print(df)
+         return(sum(df$Gini))
+     }
+     
+     NormalizedGini <- function(solution, submission) {
+         SumModelGini(solution, submission) / SumModelGini(solution, solution)
+     }    
+     
+     c(gini=NormalizedGini(data$obs,data$pred))
+ }
> 
> trControl <- trainControl(method="cv",
+                           number=10,
+                           verboseIter=TRUE,
+                           classProbs = FALSE,
+                           summaryFunction = giniSummary,
+                           savePredictions=TRUE,
+                           index = createFolds(train_set$Hazard,k=10, list=TRUE, returnTrain=TRUE),
+                           allowParallel = TRUE
+ )
> #                         allowParallel = FALSE)
> 
> model_specs <- list(
+ #    rf5=caretModelSpec(method='rf', tuneGrid=data.frame(mtry=5), ntree=50),
+     rf10=caretModelSpec(method='rf', tuneGrid=data.frame(mtry=10), ntree=150),        
+     rf20=caretModelSpec(method='rf', tuneGrid=data.frame(mtry=20), ntree=150),
+ #    rf21=caretModelSpec(method='rf', tuneGrid=data.frame(mtry=20), ntree=51),    
+     rf30=caretModelSpec(method='rf', tuneGrid=data.frame(mtry=30), ntree=150),        
+     rf50=caretModelSpec(method='rf', tuneGrid=data.frame(mtry=50), ntree=150),        
+ #    gamSpline10=caretModelSpec(method='gamSpline', tuneGrid = data.frame(df=10)),
+ #    gamSpline20=caretModelSpec(method='gamSpline', tuneGrid = data.frame(df=20)),
+ #    gamSpline30=caretModelSpec(method='gamSpline', tuneGrid = data.frame(df=30)),      
+     gamSpline31=caretModelSpec(method='gamSpline', tuneGrid = data.frame(df=31)),          
+     gamSpline50=caretModelSpec(method='gamSpline', tuneGrid = data.frame(df=50)),              
+     gamSpline100=caretModelSpec(method='gamSpline', tuneGrid = data.frame(df=100)),              
+ 
+ #    lasso0.9=caretModelSpec(method='lasso', tuneGrid = data.frame(fraction=0.9)),      
+ #    lasso0.91=caretModelSpec(method='lasso', tuneGrid = data.frame(fraction=0.91)),      
+ #    lasso0.5=caretModelSpec(method='lasso', tuneGrid = data.frame(fraction=0.5)),      
+ #    lasso0.2=caretModelSpec(method='lasso', tuneGrid = data.frame(fraction=0.2)),
+     gamLoees_1=caretModelSpec(method='gamLoess', tuneGrid = data.frame(span = .5, degree = 1)),
+ #    gamLoees_2=caretModelSpec(method='gamLoess', tuneGrid = data.frame(span = .5, degree = 2)),
+ #    xgbTree=caretModelSpec(method='xgbTree', tuneGrid = data.frame(nrounds = 500, max_depth = 5, eta=0.001)),
+     xgbTree=caretModelSpec(method='xgbTree', tuneGrid = data.frame(nrounds = 500, max_depth = 5, eta=0.01)),
+     xgbTree=caretModelSpec(method='xgbTree', tuneGrid = data.frame(nrounds = 1000, max_depth = 10, eta=0.001)),
+     xgbTree=caretModelSpec(method='xgbTree', tuneGrid = data.frame(nrounds = 1000, max_depth = 10, eta=0.005)),
+     xgbTree=caretModelSpec(method='xgbTree', tuneGrid = data.frame(nrounds = 1000, max_depth = 15, eta=0.005))
+ #    krlsPoly2_01=caretModelSpec(method='krlsPoly', tuneGrid = data.frame(degree=2,lambda=0.01)),        
+ #    krlsPoly2_001=caretModelSpec(method='krlsPoly', tuneGrid = data.frame(degree=2,lambda=0.001)),        
+ #    krlsPoly3_01=caretModelSpec(method='krlsPoly', tuneGrid = data.frame(degree=3,lambda=0.01)),        
+ #    krlsPoly3_001=caretModelSpec(method='krlsPoly', tuneGrid = data.frame(degree=3,lambda=0.001))           
+ )
> 
> y<-train_set$Hazard
> dummies <- dummyVars(~ ., data = train_set[,-1])
> x_train = predict(dummies, newdata = train_set[,-1])
> x_test = predict(dummies, newdata = test_data[,-1])
> 
> 
> cl <- makeCluster(10, outfile="")
starting worker pid=11245 on localhost:11880 at 14:46:16.458
starting worker pid=11254 on localhost:11880 at 14:46:16.722
starting worker pid=11263 on localhost:11880 at 14:46:16.970
starting worker pid=11272 on localhost:11880 at 14:46:17.216
starting worker pid=11281 on localhost:11880 at 14:46:17.486
starting worker pid=11290 on localhost:11880 at 14:46:17.749
starting worker pid=11299 on localhost:11880 at 14:46:18.010
starting worker pid=11308 on localhost:11880 at 14:46:18.265
starting worker pid=11317 on localhost:11880 at 14:46:18.519
starting worker pid=11326 on localhost:11880 at 14:46:18.770
> registerDoParallel(cl)
> 
> model_list <- caretList(x_train,
+     y,
+     trControl=trControl,
+     metric='gini',
+     tuneList=model_specs
+ )
Loading required package: randomForest
randomForest 4.6-10
Type rfNews() to see new features/changes/bug fixes.
Loading required package: caret
Loading required package: lattice
Loading required package: ggplot2
Loading required package: caret
Loading required package: lattice
Loading required package: ggplot2
Loading required package: caret
Loading required package: lattice
Loading required package: ggplot2
Loading required package: caret
Loading required package: lattice
Loading required package: ggplot2
Loading required package: caret
Loading required package: lattice
Loading required package: ggplot2
Loading required package: caret
Loading required package: lattice
Loading required package: ggplot2
Loading required package: caret
Loading required package: lattice
Loading required package: ggplot2
Loading required package: caret
Loading required package: lattice
Loading required package: ggplot2
Loading required package: caret
Loading required package: lattice
Loading required package: ggplot2
Loading required package: caret
Loading required package: lattice
Loading required package: ggplot2
loaded caret and set parent environment
randomForest 4.6-10
Type rfNews() to see new features/changes/bug fixes.
loaded caret and set parent environment
loaded caret and set parent environment
randomForest 4.6-10
Type rfNews() to see new features/changes/bug fixes.
randomForest 4.6-10
Type rfNews() to see new features/changes/bug fixes.
loaded caret and set parent environment
randomForest 4.6-10
Type rfNews() to see new features/changes/bug fixes.
loaded caret and set parent environment
randomForest 4.6-10
Type rfNews() to see new features/changes/bug fixes.
loaded caret and set parent environment
loaded caret and set parent environment
randomForest 4.6-10
Type rfNews() to see new features/changes/bug fixes.
randomForest 4.6-10
Type rfNews() to see new features/changes/bug fixes.
loaded caret and set parent environment
randomForest 4.6-10
Type rfNews() to see new features/changes/bug fixes.
loaded caret and set parent environment
randomForest 4.6-10
Type rfNews() to see new features/changes/bug fixes.
loaded caret and set parent environment
randomForest 4.6-10
Type rfNews() to see new features/changes/bug fixes.
+ Fold01: mtry=10 
+ Fold02: mtry=10 
+ Fold03: mtry=10 
+ Fold04: mtry=10 
+ Fold05: mtry=10+ Fold08: mtry=10+ Fold09: mtry=10 
+ Fold10: mtry=10 
+ Fold07: mtry=10 
+ Fold06: mtry=10 
  

- Fold05: mtry=10 
- Fold08: mtry=10 
- Fold07: mtry=10 
- Fold03: mtry=10 
- Fold02: mtry=10 
- Fold04: mtry=10 
- Fold01: mtry=10 
- Fold09: mtry=10 
- Fold06: mtry=10 
- Fold10: mtry=10 
Aggregating results
Fitting final model on full training set
[1] "saving model: cache/rf_10_150"
[1] "Loading model:cache/rf_20_150"
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
+ Fold01: mtry=30 
+ Fold05: mtry=30 
+ Fold06: mtry=30 
+ Fold02: mtry=30 
+ Fold03: mtry=30+ Fold07: mtry=30 
+ Fold08: mtry=30 
+ Fold04: mtry=30 
+ Fold10: mtry=30 
+ Fold09: mtry=30 
 
- Fold08: mtry=30 
- Fold05: mtry=30 
- Fold07: mtry=30 
- Fold09: mtry=30 
- Fold02: mtry=30 
- Fold04: mtry=30 
- Fold03: mtry=30 
- Fold01: mtry=30 
- Fold06: mtry=30 
- Fold10: mtry=30 
Aggregating results
Fitting final model on full training set
[1] "saving model: cache/rf_30_150"
[1] "Loading model:cache/rf_50_150"
[1] "Loading model:cache/gamSpline_31"
[1] "Loading model:cache/gamSpline_50"
[1] "Loading model:cache/gamSpline_100"
[1] "Loading model:cache/gamLoess_0.5_1"
[1] "Loading model:cache/xgbTree_500_5_0.01"
[1] "Loading model:cache/xgbTree_1000_10_0.001"
[1] "Loading model:cache/xgbTree_1000_10_0.005"
[1] "Loading model:cache/xgbTree_1000_15_0.005"
> 
> stopCluster(cl)
> 
> 
> xyplot(resamples(model_list))
> modelCor(resamples(model_list))
                  rf10      rf20      rf30      rf50 gamSpline31 gamSpline50
rf10         1.0000000 0.9259752 0.9349186 0.9675907   0.8262700   0.8280215
rf20         0.9259752 1.0000000 0.9512508 0.9599097   0.7543895   0.7461154
rf30         0.9349186 0.9512508 1.0000000 0.9446970   0.8580363   0.8497509
rf50         0.9675907 0.9599097 0.9446970 1.0000000   0.7736689   0.7732103
gamSpline31  0.8262700 0.7543895 0.8580363 0.7736689   1.0000000   0.9990988
gamSpline50  0.8280215 0.7461154 0.8497509 0.7732103   0.9990988   1.0000000
gamSpline100 0.8354311 0.7496114 0.8571016 0.7803518   0.9981159   0.9992859
gamLoees_1   0.8583024 0.7596009 0.8559184 0.7839363   0.9899441   0.9898027
xgbTree      0.9406018 0.9463290 0.9487980 0.9628813   0.8802742   0.8804081
xgbTree.1    0.8201014 0.8835619 0.8817101 0.8721727   0.8371721   0.8326810
xgbTree.2    0.8719426 0.8788384 0.8834665 0.8868795   0.8691483   0.8692351
xgbTree.3    0.8770480 0.7958074 0.8628074 0.8822787   0.8411397   0.8494786
             gamSpline100 gamLoees_1   xgbTree xgbTree.1 xgbTree.2 xgbTree.3
rf10            0.8354311  0.8583024 0.9406018 0.8201014 0.8719426 0.8770480
rf20            0.7496114  0.7596009 0.9463290 0.8835619 0.8788384 0.7958074
rf30            0.8571016  0.8559184 0.9487980 0.8817101 0.8834665 0.8628074
rf50            0.7803518  0.7839363 0.9628813 0.8721727 0.8868795 0.8822787
gamSpline31     0.9981159  0.9899441 0.8802742 0.8371721 0.8691483 0.8411397
gamSpline50     0.9992859  0.9898027 0.8804081 0.8326810 0.8692351 0.8494786
gamSpline100    1.0000000  0.9906505 0.8828184 0.8376176 0.8721160 0.8616873
gamLoees_1      0.9906505  1.0000000 0.8681542 0.8127326 0.8497947 0.8304696
xgbTree         0.8828184  0.8681542 1.0000000 0.9376433 0.9608334 0.9213366
xgbTree.1       0.8376176  0.8127326 0.9376433 1.0000000 0.9688349 0.8984000
xgbTree.2       0.8721160  0.8497947 0.9608334 0.9688349 1.0000000 0.9357196
xgbTree.3       0.8616873  0.8304696 0.9213366 0.8984000 0.9357196 1.0000000
> 
> greedy_ensemble <- caretEnsemble(model_list, optFUN=greedOptGini)
> summary(greedy_ensemble)
The following models were ensembled: rf20, rf30, gamSpline100, xgbTree, xgbTree.2, xgbTree.3 
They were weighted: 
0.07 0.05 0.09 0.42 0.3 0.07
The resulting RMSE is: 3.7843
The fit for each individual model on the RMSE is: 
       method   metric  metricSD
         rf20 3.810880 0.1119532
         rf30 3.814490 0.1091557
 gamSpline100 3.863440 0.1203940
      xgbTree 3.793438 0.1096350
    xgbTree.2 3.817156 0.1072673
    xgbTree.3 3.903131 0.0943993
> evalEnsemble(greedy_ensemble, NormalizedGini)
[1] 0.3814966
       rf      rf.1 gamSpline   xgbTree xgbTree.1 xgbTree.2 
0.3540378 0.3524944 0.3258668 0.3737379 0.3695134 0.3363035 
> test_pred <-predict(greedy_ensemble,newdata=x_test)
Loading required package: gam
Loading required package: splines
Loaded gam 1.09.1

Loading required package: xgboost
Loading required package: plyr
Warning message:
In predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type ==  :
  prediction from a rank-deficient fit may be misleading
> result <- data.frame(Id=test_data$Id, Hazard=test_pred)
> write.csv(result, file.path('output','submission.csv'), row.names=FALSE, quote=FALSE)
> 
> 
