
R version 3.1.3 (2015-03-09) -- "Smooth Sidewalk"
Copyright (C) 2015 The R Foundation for Statistical Computing
Platform: x86_64-unknown-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> library(caret)
Loading required package: lattice
Loading required package: ggplot2
> library(caretEnsemble)
> library(lattice)
> library(mlbench)
> library(doParallel)
Loading required package: foreach
Loading required package: iterators
Loading required package: parallel
> 
> getModelName<-function(modelSpec) {
+     paste(unlist(modelSpec), sep='_', collapse="_")
+ }
> 
> #"NormalizedGini" is the other half of the metric. This function does most of the work, though
> SumModelGini <- function(solution, submission) {
+     df = data.frame(solution = solution, submission = submission)
+     df <- df[order(df$submission, decreasing = TRUE),]
+     df
+     df$random = (1:nrow(df))/nrow(df)
+     df
+     totalPos <- sum(df$solution)
+     df$cumPosFound <- cumsum(df$solution) # this will store the cumulative number of positive examples found (used for computing "Model Lorentz")
+     df$Lorentz <- df$cumPosFound / totalPos # this will store the cumulative proportion of positive examples found ("Model Lorentz")
+     df$Gini <- df$Lorentz - df$random # will store Lorentz minus random
+     #print(df)
+     return(sum(df$Gini))
+ }
> 
> NormalizedGini <- function(solution, submission) {
+     SumModelGini(solution, submission) / SumModelGini(solution, solution)
+ }
> 
> greedOptGini <- function(X, Y, iter = 100L){
+     
+     N           <- ncol(X)
+     weights     <- rep(0L, N)
+     pred        <- 0 * X
+     sum.weights <- 0L
+     
+     while(sum.weights < iter) {
+         
+         sum.weights   <- sum.weights + 1L
+         pred          <- (pred + X) * (1L / sum.weights)
+         errors        <- apply(pred, MARGIN=2,function(x) {NormalizedGini(Y,x)})
+         best          <- which.max(errors)
+         weights[best] <- weights[best] + 1L
+         pred          <- pred[, best] * sum.weights
+     }
+     return(weights)
+ }
> 
> printf <- function(...) invisible(print(sprintf(...)))
> 
> train_data<- read.csv('data/train.csv', header=TRUE)
> test_data <- read.csv('data/test.csv', header=TRUE)
> #train_set <- subset(train_data,select=-c(Id,T1_V4,T1_V5,T1_V15,T1_V16, T1_V11))
> trainIdx <-createDataPartition(train_data$Hazard, p=0.01, list=FALSE)
> 
> train_set <- subset(train_data,select=-c(Id))
> 
> set.seed(37)
> 
> giniSummary <-function (data, lev = NULL, model = NULL) {
+ 
+     SumModelGini <- function(solution, submission) {
+         df = data.frame(solution = solution, submission = submission)
+         df <- df[order(df$submission, decreasing = TRUE),]
+         df
+         df$random = (1:nrow(df))/nrow(df)
+         df
+         totalPos <- sum(df$solution)
+         df$cumPosFound <- cumsum(df$solution) # this will store the cumulative number of positive examples found (used for computing "Model Lorentz")
+         df$Lorentz <- df$cumPosFound / totalPos # this will store the cumulative proportion of positive examples found ("Model Lorentz")
+         df$Gini <- df$Lorentz - df$random # will store Lorentz minus random
+         #print(df)
+         return(sum(df$Gini))
+     }
+     
+     NormalizedGini <- function(solution, submission) {
+         SumModelGini(solution, submission) / SumModelGini(solution, solution)
+     }    
+     
+     c(gini=NormalizedGini(data$obs,data$pred))
+ }
> 
> trControl <- trainControl(method="cv",
+                           number=10,
+                           verboseIter=TRUE,
+                           classProbs = FALSE,
+                           summaryFunction = giniSummary,
+                           savePredictions=TRUE,
+                           index = createFolds(train_set$Hazard,k=10, list=TRUE, returnTrain=TRUE),
+                           allowParallel = TRUE
+ )
> #                         allowParallel = FALSE)
> 
> 
> caretModelSpecEx<-function(method,tuneGrid,...) {
+     l <- apply(tuneGrid,MARGIN=1, FUN=function(x) {caretModelSpec(method, tuneGrid=as.data.frame(as.list(x)),...)})
+     names(l) <-apply(tuneGrid, MARGIN=1, function(r) {paste(c('rf', r), collapse='_' )})
+     l
+ }
> 
> 
> model_specs <- c(
+   #list(
+ #    rf5=caretModelSpec(method='rf', tuneGrid=data.frame(mtry=5), ntree=50),
+    # rf10=caretModelSpec(method='rf', tuneGrid=data.frame(mtry=10), ntree=150),        
+    # rf20=caretModelSpec(method='rf', tuneGrid=data.frame(mtry=20), ntree=150),
+ #    rf21=caretModelSpec(method='rf', tuneGrid=data.frame(mtry=20), ntree=51),    
+    # rf30=caretModelSpec(method='rf', tuneGrid=data.frame(mtry=30), ntree=150),        
+    # rf50=caretModelSpec(method='rf', tuneGrid=data.frame(mtry=50), ntree=150),        
+ #    gamSpline10=caretModelSpec(method='gamSpline', tuneGrid = data.frame(df=10)),
+ #    gamSpline20=caretModelSpec(method='gamSpline', tuneGrid = data.frame(df=20)),
+ #    gamSpline30=caretModelSpec(method='gamSpline', tuneGrid = data.frame(df=30)),      
+    # gamSpline31=caretModelSpec(method='gamSpline', tuneGrid = data.frame(df=31)),          
+    # gamSpline50=caretModelSpec(method='gamSpline', tuneGrid = data.frame(df=50)),              
+    # gamSpline100=caretModelSpec(method='gamSpline', tuneGrid = data.frame(df=100)),              
+ 
+ #    lasso0.9=caretModelSpec(method='lasso', tuneGrid = data.frame(fraction=0.9)),      
+ #    lasso0.91=caretModelSpec(method='lasso', tuneGrid = data.frame(fraction=0.91)),      
+ #    lasso0.5=caretModelSpec(method='lasso', tuneGrid = data.frame(fraction=0.5)),      
+ #    lasso0.2=caretModelSpec(method='lasso', tuneGrid = data.frame(fraction=0.2)),
+    # gamLoees_1=caretModelSpec(method='gamLoess', tuneGrid = data.frame(span = .5, degree = 1))
+     #),
+ #    gamLoees_2=caretModelSpec(method='gamLoess', tuneGrid = data.frame(span = .5, degree = 2)),
+ #    xgbTree=caretModelSpec(method='xgbTree', tuneGrid = data.frame(nrounds = 500, max_depth = 5, eta=0.001)),
+     #xgbTree=caretModelSpec(method='xgbTree', tuneGrid = data.frame(nrounds = 500, max_depth = 5, eta=0.01)),
+     #xgbTree=caretModelSpec(method='xgbTree', tuneGrid = data.frame(nrounds = 1000, max_depth = 10, eta=0.001)),
+     #xgbTree=caretModelSpec(method='xgbTree', tuneGrid = data.frame(nrounds = 1000, max_depth = 10, eta=0.005)),
+     #xgbTree=caretModelSpec(method='xgbTree', tuneGrid = data.frame(nrounds = 1000, max_depth = 15, eta=0.005))
+     caretModelSpecEx(method='xgbTree', tuneGrid = expand.grid(nrounds = c(2000), max_depth = 5, eta=c(0.01)))
+ #    krlsPoly2_01=caretModelSpec(method='krlsPoly', tuneGrid = data.frame(degree=2,lambda=0.01)),        
+ #    krlsPoly2_001=caretModelSpec(method='krlsPoly', tuneGrid = data.frame(degree=2,lambda=0.001)),        
+ #    krlsPoly3_01=caretModelSpec(method='krlsPoly', tuneGrid = data.frame(degree=3,lambda=0.01)),        
+ #    krlsPoly3_001=caretModelSpec(method='krlsPoly', tuneGrid = data.frame(degree=3,lambda=0.001))           
+ )
> 
> y<-train_set$Hazard
> dummies <- dummyVars(~ ., data = train_set[,-1])
> x_train = predict(dummies, newdata = train_set[,-1])
> x_test = predict(dummies, newdata = test_data[,-1])
> 
> 
> cl <- makeCluster(10, outfile="")
starting worker pid=5395 on localhost:11357 at 20:43:01.076
starting worker pid=5404 on localhost:11357 at 20:43:01.326
starting worker pid=5413 on localhost:11357 at 20:43:01.585
starting worker pid=5422 on localhost:11357 at 20:43:01.835
starting worker pid=5431 on localhost:11357 at 20:43:02.084
starting worker pid=5440 on localhost:11357 at 20:43:02.330
starting worker pid=5449 on localhost:11357 at 20:43:02.585
starting worker pid=5458 on localhost:11357 at 20:43:02.832
starting worker pid=5467 on localhost:11357 at 20:43:03.077
starting worker pid=5476 on localhost:11357 at 20:43:03.326
> registerDoParallel(cl)
> 
> model_list <- caretList(x_train,
+     y,
+     trControl=trControl,
+     metric='gini',
+     tuneList=model_specs
+ )
[1] "Loading model:cache/xgbTree_2000_5_0.01"
> 
> stopCluster(cl)
> 
> 
> xyplot(resamples(model_list))
Error in resamples.default(model_list) : 
  at least two train objects are needed
Calls: xyplot -> resamples -> resamples.default
Execution halted
