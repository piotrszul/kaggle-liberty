
R version 3.1.3 (2015-03-09) -- "Smooth Sidewalk"
Copyright (C) 2015 The R Foundation for Statistical Computing
Platform: x86_64-unknown-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> library(caret)
Loading required package: lattice
Loading required package: ggplot2
> library(caretEnsemble)
> library(lattice)
> library(mlbench)
> library(doParallel)
Loading required package: foreach
Loading required package: iterators
Loading required package: parallel
> 
> getModelName<-function(modelSpec) {
+     paste(unlist(modelSpec), sep='_', collapse="_")
+ }
> 
> #"NormalizedGini" is the other half of the metric. This function does most of the work, though
> SumModelGini <- function(solution, submission) {
+     df = data.frame(solution = solution, submission = submission)
+     df <- df[order(df$submission, decreasing = TRUE),]
+     df
+     df$random = (1:nrow(df))/nrow(df)
+     df
+     totalPos <- sum(df$solution)
+     df$cumPosFound <- cumsum(df$solution) # this will store the cumulative number of positive examples found (used for computing "Model Lorentz")
+     df$Lorentz <- df$cumPosFound / totalPos # this will store the cumulative proportion of positive examples found ("Model Lorentz")
+     df$Gini <- df$Lorentz - df$random # will store Lorentz minus random
+     #print(df)
+     return(sum(df$Gini))
+ }
> 
> NormalizedGini <- function(solution, submission) {
+     SumModelGini(solution, submission) / SumModelGini(solution, solution)
+ }
> 
> greedOptGini <- function(X, Y, iter = 100L){
+     
+     N           <- ncol(X)
+     weights     <- rep(0L, N)
+     pred        <- 0 * X
+     sum.weights <- 0L
+     
+     while(sum.weights < iter) {
+         
+         sum.weights   <- sum.weights + 1L
+         pred          <- (pred + X) * (1L / sum.weights)
+         errors        <- apply(pred, MARGIN=2,function(x) {NormalizedGini(Y,x)})
+         best          <- which.max(errors)
+         weights[best] <- weights[best] + 1L
+         pred          <- pred[, best] * sum.weights
+     }
+     return(weights)
+ }
> 
> printf <- function(...) invisible(print(sprintf(...)))
> 
> train_data<- read.csv('data/train.csv', header=TRUE)
> test_data <- read.csv('data/test.csv', header=TRUE)
> #train_set <- subset(train_data,select=-c(Id,T1_V4,T1_V5,T1_V15,T1_V16, T1_V11))
> trainIdx <-createDataPartition(train_data$Hazard, p=0.01, list=FALSE)
> 
> train_set <- subset(train_data,select=-c(Id))
> 
> set.seed(37)
> 
> giniSummary <-function (data, lev = NULL, model = NULL) {
+ 
+     SumModelGini <- function(solution, submission) {
+         df = data.frame(solution = solution, submission = submission)
+         df <- df[order(df$submission, decreasing = TRUE),]
+         df
+         df$random = (1:nrow(df))/nrow(df)
+         df
+         totalPos <- sum(df$solution)
+         df$cumPosFound <- cumsum(df$solution) # this will store the cumulative number of positive examples found (used for computing "Model Lorentz")
+         df$Lorentz <- df$cumPosFound / totalPos # this will store the cumulative proportion of positive examples found ("Model Lorentz")
+         df$Gini <- df$Lorentz - df$random # will store Lorentz minus random
+         #print(df)
+         return(sum(df$Gini))
+     }
+     
+     NormalizedGini <- function(solution, submission) {
+         SumModelGini(solution, submission) / SumModelGini(solution, solution)
+     }    
+     
+     c(gini=NormalizedGini(data$obs,data$pred))
+ }
> 
> trControl <- trainControl(method="cv",
+                           number=10,
+                           verboseIter=TRUE,
+                           classProbs = FALSE,
+                           summaryFunction = giniSummary,
+                           savePredictions=TRUE,
+                           index = createFolds(train_set$Hazard,k=10, list=TRUE, returnTrain=TRUE),
+                           allowParallel = TRUE
+ )
> #                         allowParallel = FALSE)
> 
> model_specs <- list(
+ #    rf5=caretModelSpec(method='rf', tuneGrid=data.frame(mtry=5), ntree=50),
+ #    rf10=caretModelSpec(method='rf', tuneGrid=data.frame(mtry=10), ntree=50),        
+     rf20=caretModelSpec(method='rf', tuneGrid=data.frame(mtry=20), ntree=150),
+ #    rf21=caretModelSpec(method='rf', tuneGrid=data.frame(mtry=20), ntree=51),    
+     rf30=caretModelSpec(method='rf', tuneGrid=data.frame(mtry=30), ntree=100),        
+     rf50=caretModelSpec(method='rf', tuneGrid=data.frame(mtry=50), ntree=150),        
+ #    gamSpline10=caretModelSpec(method='gamSpline', tuneGrid = data.frame(df=10)),
+ #    gamSpline20=caretModelSpec(method='gamSpline', tuneGrid = data.frame(df=20)),
+ #    gamSpline30=caretModelSpec(method='gamSpline', tuneGrid = data.frame(df=30)),      
+     gamSpline31=caretModelSpec(method='gamSpline', tuneGrid = data.frame(df=31)),          
+     gamSpline50=caretModelSpec(method='gamSpline', tuneGrid = data.frame(df=50)),              
+     gamSpline100=caretModelSpec(method='gamSpline', tuneGrid = data.frame(df=100)),              
+ 
+ #    lasso0.9=caretModelSpec(method='lasso', tuneGrid = data.frame(fraction=0.9)),      
+ #    lasso0.91=caretModelSpec(method='lasso', tuneGrid = data.frame(fraction=0.91)),      
+ #    lasso0.5=caretModelSpec(method='lasso', tuneGrid = data.frame(fraction=0.5)),      
+ #    lasso0.2=caretModelSpec(method='lasso', tuneGrid = data.frame(fraction=0.2)),
+     gamLoees_1=caretModelSpec(method='gamLoess', tuneGrid = data.frame(span = .5, degree = 1)),
+ #    gamLoees_2=caretModelSpec(method='gamLoess', tuneGrid = data.frame(span = .5, degree = 2)),
+ #    xgbTree=caretModelSpec(method='xgbTree', tuneGrid = data.frame(nrounds = 500, max_depth = 5, eta=0.001)),
+     xgbTree=caretModelSpec(method='xgbTree', tuneGrid = data.frame(nrounds = 500, max_depth = 5, eta=0.01)),
+     xgbTree=caretModelSpec(method='xgbTree', tuneGrid = data.frame(nrounds = 1000, max_depth = 10, eta=0.001)),
+     xgbTree=caretModelSpec(method='xgbTree', tuneGrid = data.frame(nrounds = 1000, max_depth = 10, eta=0.005)),
+     xgbTree=caretModelSpec(method='xgbTree', tuneGrid = data.frame(nrounds = 1000, max_depth = 15, eta=0.005))
+ #    krlsPoly2_01=caretModelSpec(method='krlsPoly', tuneGrid = data.frame(degree=2,lambda=0.01)),        
+ #    krlsPoly2_001=caretModelSpec(method='krlsPoly', tuneGrid = data.frame(degree=2,lambda=0.001)),        
+ #    krlsPoly3_01=caretModelSpec(method='krlsPoly', tuneGrid = data.frame(degree=3,lambda=0.01)),        
+ #    krlsPoly3_001=caretModelSpec(method='krlsPoly', tuneGrid = data.frame(degree=3,lambda=0.001))           
+ )
> 
> y<-train_set$Hazard
> dummies <- dummyVars(~ ., data = train_set[,-1])
> x_train = predict(dummies, newdata = train_set[,-1])
> x_test = predict(dummies, newdata = test_data[,-1])
> 
> 
> cl <- makeCluster(10, outfile="")
starting worker pid=29763 on localhost:11640 at 13:15:58.933
starting worker pid=29772 on localhost:11640 at 13:15:59.185
starting worker pid=29781 on localhost:11640 at 13:15:59.438
starting worker pid=29790 on localhost:11640 at 13:15:59.709
starting worker pid=29799 on localhost:11640 at 13:15:59.978
starting worker pid=29808 on localhost:11640 at 13:16:00.232
starting worker pid=29817 on localhost:11640 at 13:16:00.486
starting worker pid=29826 on localhost:11640 at 13:16:00.739
starting worker pid=29835 on localhost:11640 at 13:16:00.988
starting worker pid=29844 on localhost:11640 at 13:16:01.240
> registerDoParallel(cl)
> 
> model_list <- caretList(x_train,
+     y,
+     trControl=trControl,
+     metric='gini',
+     tuneList=model_specs
+ )
Loading required package: randomForest
randomForest 4.6-10
Type rfNews() to see new features/changes/bug fixes.
Loading required package: caret
Loading required package: lattice
Loading required package: ggplot2
Loading required package: caret
Loading required package: lattice
Loading required package: ggplot2
Loading required package: caret
Loading required package: lattice
Loading required package: ggplot2
Loading required package: caret
Loading required package: lattice
Loading required package: ggplot2
Loading required package: caret
Loading required package: lattice
Loading required package: ggplot2
Loading required package: caret
Loading required package: lattice
Loading required package: ggplot2
Loading required package: caret
Loading required package: lattice
Loading required package: ggplot2
Loading required package: caret
Loading required package: lattice
Loading required package: ggplot2
Loading required package: caret
Loading required package: lattice
Loading required package: ggplot2
Loading required package: caret
Loading required package: lattice
Loading required package: ggplot2
loaded caret and set parent environment
randomForest 4.6-10
Type rfNews() to see new features/changes/bug fixes.
loaded caret and set parent environment
randomForest 4.6-10
Type rfNews() to see new features/changes/bug fixes.
loaded caret and set parent environment
randomForest 4.6-10
Type rfNews() to see new features/changes/bug fixes.
loaded caret and set parent environment
randomForest 4.6-10
Type rfNews() to see new features/changes/bug fixes.
loaded caret and set parent environment
randomForest 4.6-10
Type rfNews() to see new features/changes/bug fixes.
loaded caret and set parent environment
randomForest 4.6-10
Type rfNews() to see new features/changes/bug fixes.
loaded caret and set parent environment
randomForest 4.6-10
Type rfNews() to see new features/changes/bug fixes.
loaded caret and set parent environment
randomForest 4.6-10
Type rfNews() to see new features/changes/bug fixes.
loaded caret and set parent environment
randomForest 4.6-10
Type rfNews() to see new features/changes/bug fixes.
loaded caret and set parent environment
randomForest 4.6-10
Type rfNews() to see new features/changes/bug fixes.
+ Fold01: mtry=20+ Fold03: mtry=20+ Fold05: mtry=20+ Fold04: mtry=20 
+ Fold07: mtry=20 
+ Fold10: mtry=20 
+ Fold06: mtry=20 
+ Fold02: mtry=20 
+ Fold08: mtry=20 
  + Fold09: mtry=20 
 


- Fold05: mtry=20 
- Fold07: mtry=20 
- Fold08: mtry=20 
- Fold02: mtry=20 
- Fold03: mtry=20 
- Fold04: mtry=20 
- Fold06: mtry=20 
- Fold09: mtry=20 
- Fold01: mtry=20 
- Fold10: mtry=20 
Aggregating results
Fitting final model on full training set
[1] "saving model: cache/rf_20_150"
[1] "Loading model:cache/rf_30_100"
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
+ Fold01: mtry=50+ Fold02: mtry=50 
+ Fold05: mtry=50 
+ Fold06: mtry=50 
+ Fold07: mtry=50 
+ Fold03: mtry=50 
 + Fold04: mtry=50 
+ Fold08: mtry=50 
+ Fold09: mtry=50 
+ Fold10: mtry=50 

- Fold05: mtry=50 
- Fold08: mtry=50 
- Fold07: mtry=50 
- Fold06: mtry=50 
- Fold02: mtry=50 
- Fold04: mtry=50 
- Fold03: mtry=50 
- Fold09: mtry=50 
- Fold01: mtry=50 
- Fold10: mtry=50 
Aggregating results
Fitting final model on full training set
[1] "saving model: cache/rf_50_150"
[1] "Loading model:cache/gamSpline_31"
[1] "Loading model:cache/gamSpline_50"
[1] "Loading model:cache/gamSpline_100"
[1] "Loading model:cache/gamLoess_0.5_1"
[1] "Loading model:cache/xgbTree_500_5_0.01"
[1] "Loading model:cache/xgbTree_1000_10_0.001"
[1] "Loading model:cache/xgbTree_1000_10_0.005"
[1] "Loading model:cache/xgbTree_1000_15_0.005"
> 
> stopCluster(cl)
> 
> 
> xyplot(resamples(model_list))
> modelCor(resamples(model_list))
                  rf20      rf30      rf50 gamSpline31 gamSpline50 gamSpline100
rf20         1.0000000 0.9256087 0.9599097   0.7543895   0.7461154    0.7496114
rf30         0.9256087 1.0000000 0.9698084   0.7829153   0.7836204    0.7858790
rf50         0.9599097 0.9698084 1.0000000   0.7736689   0.7732103    0.7803518
gamSpline31  0.7543895 0.7829153 0.7736689   1.0000000   0.9990988    0.9981159
gamSpline50  0.7461154 0.7836204 0.7732103   0.9990988   1.0000000    0.9992859
gamSpline100 0.7496114 0.7858790 0.7803518   0.9981159   0.9992859    1.0000000
gamLoees_1   0.7596009 0.7781007 0.7839363   0.9899441   0.9898027    0.9906505
xgbTree      0.9463290 0.9510864 0.9628813   0.8802742   0.8804081    0.8828184
xgbTree.1    0.8835619 0.8154226 0.8721727   0.8371721   0.8326810    0.8376176
xgbTree.2    0.8788384 0.8611540 0.8868795   0.8691483   0.8692351    0.8721160
xgbTree.3    0.7958074 0.8464228 0.8822787   0.8411397   0.8494786    0.8616873
             gamLoees_1   xgbTree xgbTree.1 xgbTree.2 xgbTree.3
rf20          0.7596009 0.9463290 0.8835619 0.8788384 0.7958074
rf30          0.7781007 0.9510864 0.8154226 0.8611540 0.8464228
rf50          0.7839363 0.9628813 0.8721727 0.8868795 0.8822787
gamSpline31   0.9899441 0.8802742 0.8371721 0.8691483 0.8411397
gamSpline50   0.9898027 0.8804081 0.8326810 0.8692351 0.8494786
gamSpline100  0.9906505 0.8828184 0.8376176 0.8721160 0.8616873
gamLoees_1    1.0000000 0.8681542 0.8127326 0.8497947 0.8304696
xgbTree       0.8681542 1.0000000 0.9376433 0.9608334 0.9213366
xgbTree.1     0.8127326 0.9376433 1.0000000 0.9688349 0.8984000
xgbTree.2     0.8497947 0.9608334 0.9688349 1.0000000 0.9357196
xgbTree.3     0.8304696 0.9213366 0.8984000 0.9357196 1.0000000
> 
> greedy_ensemble <- caretEnsemble(model_list, optFUN=greedOptGini)
> summary(greedy_ensemble)
The following models were ensembled: rf20, rf30, gamSpline100, xgbTree, xgbTree.2, xgbTree.3 
They were weighted: 
0.09 0.02 0.09 0.43 0.3 0.07
The resulting RMSE is: 3.7846
The fit for each individual model on the RMSE is: 
       method   metric  metricSD
         rf20 3.810880 0.1119532
         rf30 3.824924 0.1098254
 gamSpline100 3.863440 0.1203940
      xgbTree 3.793438 0.1096350
    xgbTree.2 3.817156 0.1072673
    xgbTree.3 3.903131 0.0943993
> evalEnsemble(greedy_ensemble, NormalizedGini)
[1] 0.3814454
       rf      rf.1 gamSpline   xgbTree xgbTree.1 xgbTree.2 
0.3540378 0.3466169 0.3258668 0.3737379 0.3695134 0.3363035 
> test_pred <-predict(greedy_ensemble,newdata=x_test)
Loading required package: gam
Loading required package: splines
Loaded gam 1.09.1

Loading required package: xgboost
Loading required package: plyr
Warning message:
In predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type ==  :
  prediction from a rank-deficient fit may be misleading
> result <- data.frame(Id=test_data$Id, Hazard=test_pred)
> write.csv(result, file.path('output','submission.csv'), row.names=FALSE, quote=FALSE)
> 
> 
