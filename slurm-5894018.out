
R version 3.1.3 (2015-03-09) -- "Smooth Sidewalk"
Copyright (C) 2015 The R Foundation for Statistical Computing
Platform: x86_64-unknown-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> library(caret)
Loading required package: lattice
Loading required package: ggplot2
> library(caretEnsemble)
> library(lattice)
> library(mlbench)
> library(doParallel)
Loading required package: foreach
Loading required package: iterators
Loading required package: parallel
> 
> getModelName<-function(modelSpec) {
+     paste(unlist(modelSpec), sep='_', collapse="_")
+ }
> 
> #"NormalizedGini" is the other half of the metric. This function does most of the work, though
> SumModelGini <- function(solution, submission) {
+     df = data.frame(solution = solution, submission = submission)
+     df <- df[order(df$submission, decreasing = TRUE),]
+     df
+     df$random = (1:nrow(df))/nrow(df)
+     df
+     totalPos <- sum(df$solution)
+     df$cumPosFound <- cumsum(df$solution) # this will store the cumulative number of positive examples found (used for computing "Model Lorentz")
+     df$Lorentz <- df$cumPosFound / totalPos # this will store the cumulative proportion of positive examples found ("Model Lorentz")
+     df$Gini <- df$Lorentz - df$random # will store Lorentz minus random
+     #print(df)
+     return(sum(df$Gini))
+ }
> 
> NormalizedGini <- function(solution, submission) {
+     SumModelGini(solution, submission) / SumModelGini(solution, solution)
+ }
> 
> greedOptGini <- function(X, Y, iter = 100L){
+     
+     N           <- ncol(X)
+     weights     <- rep(0L, N)
+     pred        <- 0 * X
+     sum.weights <- 0L
+     
+     while(sum.weights < iter) {
+         
+         sum.weights   <- sum.weights + 1L
+         pred          <- (pred + X) * (1L / sum.weights)
+         errors        <- apply(pred, MARGIN=2,function(x) {NormalizedGini(Y,x)})
+         best          <- which.max(errors)
+         weights[best] <- weights[best] + 1L
+         pred          <- pred[, best] * sum.weights
+     }
+     return(weights)
+ }
> 
> printf <- function(...) invisible(print(sprintf(...)))
> 
> train_data<- read.csv('data/train.csv', header=TRUE)
> test_data <- read.csv('data/test.csv', header=TRUE)
> #train_set <- subset(train_data,select=-c(Id,T1_V4,T1_V5,T1_V15,T1_V16, T1_V11))
> trainIdx <-createDataPartition(train_data$Hazard, p=0.01, list=FALSE)
> 
> train_set <- subset(train_data,select=-c(Id))
> 
> set.seed(37)
> 
> giniSummary <-function (data, lev = NULL, model = NULL) {
+ 
+     SumModelGini <- function(solution, submission) {
+         df = data.frame(solution = solution, submission = submission)
+         df <- df[order(df$submission, decreasing = TRUE),]
+         df
+         df$random = (1:nrow(df))/nrow(df)
+         df
+         totalPos <- sum(df$solution)
+         df$cumPosFound <- cumsum(df$solution) # this will store the cumulative number of positive examples found (used for computing "Model Lorentz")
+         df$Lorentz <- df$cumPosFound / totalPos # this will store the cumulative proportion of positive examples found ("Model Lorentz")
+         df$Gini <- df$Lorentz - df$random # will store Lorentz minus random
+         #print(df)
+         return(sum(df$Gini))
+     }
+     
+     NormalizedGini <- function(solution, submission) {
+         SumModelGini(solution, submission) / SumModelGini(solution, solution)
+     }    
+     
+     c(gini=NormalizedGini(data$obs,data$pred))
+ }
> 
> trControl <- trainControl(method="cv",
+                           number=10,
+                           verboseIter=TRUE,
+                           classProbs = FALSE,
+                           summaryFunction = giniSummary,
+                           savePredictions=TRUE,
+                           index = createFolds(train_set$Hazard,k=10, list=TRUE, returnTrain=TRUE),
+                           allowParallel = TRUE
+ )
> #                         allowParallel = FALSE)
> 
> model_specs <- list(
+ #    rf5=caretModelSpec(method='rf', tuneGrid=data.frame(mtry=5), ntree=50),
+ #    rf10=caretModelSpec(method='rf', tuneGrid=data.frame(mtry=10), ntree=50),        
+ #    rf20=caretModelSpec(method='rf', tuneGrid=data.frame(mtry=20), ntree=50),
+ #    rf21=caretModelSpec(method='rf', tuneGrid=data.frame(mtry=20), ntree=51),    
+     rf31=caretModelSpec(method='rf', tuneGrid=data.frame(mtry=30), ntree=100),        
+ #    gamSpline10=caretModelSpec(method='gamSpline', tuneGrid = data.frame(df=10)),
+ #    gamSpline20=caretModelSpec(method='gamSpline', tuneGrid = data.frame(df=20)),
+ #    gamSpline30=caretModelSpec(method='gamSpline', tuneGrid = data.frame(df=30)),      
+     gamSpline31=caretModelSpec(method='gamSpline', tuneGrid = data.frame(df=31)),          
+     gamSpline50=caretModelSpec(method='gamSpline', tuneGrid = data.frame(df=50)),              
+     gamSpline100=caretModelSpec(method='gamSpline', tuneGrid = data.frame(df=100)),              
+ 
+ #    lasso0.9=caretModelSpec(method='lasso', tuneGrid = data.frame(fraction=0.9)),      
+ #    lasso0.91=caretModelSpec(method='lasso', tuneGrid = data.frame(fraction=0.91)),      
+ #    lasso0.5=caretModelSpec(method='lasso', tuneGrid = data.frame(fraction=0.5)),      
+ #    lasso0.2=caretModelSpec(method='lasso', tuneGrid = data.frame(fraction=0.2)),
+     gamLoees_1=caretModelSpec(method='gamLoess', tuneGrid = data.frame(span = .5, degree = 1)),
+ #    gamLoees_2=caretModelSpec(method='gamLoess', tuneGrid = data.frame(span = .5, degree = 2)),
+ #    xgbTree=caretModelSpec(method='xgbTree', tuneGrid = data.frame(nrounds = 500, max_depth = 5, eta=0.001)),
+     xgbTree=caretModelSpec(method='xgbTree', tuneGrid = data.frame(nrounds = 500, max_depth = 5, eta=0.01)),
+     xgbTree=caretModelSpec(method='xgbTree', tuneGrid = data.frame(nrounds = 1000, max_depth = 10, eta=0.001)),
+     xgbTree=caretModelSpec(method='xgbTree', tuneGrid = data.frame(nrounds = 1000, max_depth = 10, eta=0.005)),
+     xgbTree=caretModelSpec(method='xgbTree', tuneGrid = data.frame(nrounds = 1000, max_depth = 15, eta=0.005))
+ #    krlsPoly2_01=caretModelSpec(method='krlsPoly', tuneGrid = data.frame(degree=2,lambda=0.01)),        
+ #    krlsPoly2_001=caretModelSpec(method='krlsPoly', tuneGrid = data.frame(degree=2,lambda=0.001)),        
+ #    krlsPoly3_01=caretModelSpec(method='krlsPoly', tuneGrid = data.frame(degree=3,lambda=0.01)),        
+ #    krlsPoly3_001=caretModelSpec(method='krlsPoly', tuneGrid = data.frame(degree=3,lambda=0.001))           
+ )
> 
> y<-train_set$Hazard
> dummies <- dummyVars(~ ., data = train_set[,-1])
> x_train = predict(dummies, newdata = train_set[,-1])
> x_test = predict(dummies, newdata = test_data[,-1])
> 
> 
> cl <- makeCluster(10, outfile="")
starting worker pid=2365 on localhost:11754 at 11:54:59.198
starting worker pid=2374 on localhost:11754 at 11:54:59.453
starting worker pid=2383 on localhost:11754 at 11:54:59.707
starting worker pid=2392 on localhost:11754 at 11:54:59.964
starting worker pid=2401 on localhost:11754 at 11:55:00.216
starting worker pid=2410 on localhost:11754 at 11:55:00.458
starting worker pid=2419 on localhost:11754 at 11:55:00.709
starting worker pid=2429 on localhost:11754 at 11:55:00.964
starting worker pid=2444 on localhost:11754 at 11:55:01.213
starting worker pid=2453 on localhost:11754 at 11:55:01.464
> registerDoParallel(cl)
> 
> model_list <- caretList(x_train,
+     y,
+     trControl=trControl,
+     metric='gini',
+     tuneList=model_specs
+ )
Loading required package: randomForest
randomForest 4.6-10
Type rfNews() to see new features/changes/bug fixes.
Loading required package: caret
Loading required package: lattice
Loading required package: ggplot2
Loading required package: caret
Loading required package: lattice
Loading required package: ggplot2
Loading required package: caret
Loading required package: lattice
Loading required package: ggplot2
Loading required package: caret
Loading required package: lattice
Loading required package: ggplot2
Loading required package: caret
Loading required package: lattice
Loading required package: ggplot2
Loading required package: caret
Loading required package: lattice
Loading required package: ggplot2
Loading required package: caret
Loading required package: lattice
Loading required package: ggplot2
Loading required package: caret
Loading required package: lattice
Loading required package: ggplot2
Loading required package: caret
Loading required package: lattice
Loading required package: ggplot2
Loading required package: caret
Loading required package: lattice
Loading required package: ggplot2
loaded caret and set parent environment
randomForest 4.6-10
Type rfNews() to see new features/changes/bug fixes.
loaded caret and set parent environment
randomForest 4.6-10
Type rfNews() to see new features/changes/bug fixes.
loaded caret and set parent environment
randomForest 4.6-10
Type rfNews() to see new features/changes/bug fixes.
loaded caret and set parent environment
randomForest 4.6-10
Type rfNews() to see new features/changes/bug fixes.
loaded caret and set parent environment
randomForest 4.6-10
Type rfNews() to see new features/changes/bug fixes.
loaded caret and set parent environment
randomForest 4.6-10
Type rfNews() to see new features/changes/bug fixes.
loaded caret and set parent environment
randomForest 4.6-10
Type rfNews() to see new features/changes/bug fixes.
loaded caret and set parent environment
randomForest 4.6-10
Type rfNews() to see new features/changes/bug fixes.
loaded caret and set parent environment
loaded caret and set parent environment
randomForest 4.6-10
Type rfNews() to see new features/changes/bug fixes.
randomForest 4.6-10
Type rfNews() to see new features/changes/bug fixes.
+ Fold02: mtry=30+ Fold01: mtry=30  

+ Fold07: mtry=30 
+ Fold04: mtry=30 
+ Fold08: mtry=30 
+ Fold06: mtry=30 
+ Fold09: mtry=30 
+ Fold05: mtry=30 
+ Fold10: mtry=30 
+ Fold03: mtry=30 
- Fold05: mtry=30 
- Fold08: mtry=30 
- Fold04: mtry=30 
- Fold07: mtry=30 
- Fold02: mtry=30 
- Fold06: mtry=30 
- Fold03: mtry=30 
- Fold09: mtry=30 
- Fold01: mtry=30 
- Fold10: mtry=30 
Aggregating results
Fitting final model on full training set
[1] "saving model: cache/rf_30_100"
Loading required package: gam
Loading required package: splines
Loaded gam 1.09.1

loaded caret and set parent environment
Loading required package: splines
Loaded gam 1.09.1

loaded caret and set parent environment
Loading required package: splines
Loaded gam 1.09.1

loaded caret and set parent environment
Loading required package: splines
Loaded gam 1.09.1

loaded caret and set parent environment
Loading required package: splines
Loaded gam 1.09.1

loaded caret and set parent environment
Loading required package: splines
Loaded gam 1.09.1

loaded caret and set parent environment
Loading required package: splines
Loaded gam 1.09.1

loaded caret and set parent environment
Loading required package: splines
Loaded gam 1.09.1

loaded caret and set parent environment
Loading required package: splines
Loaded gam 1.09.1

loaded caret and set parent environment
Loading required package: splines
Loaded gam 1.09.1

loaded caret and set parent environment
Loading required package: splines
Loaded gam 1.09.1

+ Fold01: df=31+ Fold02: df=31 
+ Fold03: df=31 
 
+ Fold04: df=31+ Fold05: df=31 
+ Fold06: df=31+ Fold07: df=31 
+ Fold09: df=31+ Fold08: df=31+ Fold10: df=31     




- Fold06: df=31 
- Fold08: df=31 
- Fold10: df=31 
- Fold04: df=31 
- Fold02: df=31 
- Fold01: df=31 
- Fold05: df=31 
- Fold09: df=31 
- Fold07: df=31 
- Fold03: df=31 
Aggregating results
Fitting final model on full training set
[1] "saving model: cache/gamSpline_31"
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
+ Fold02: df=50 
+ Fold03: df=50 
+ Fold05: df=50 
+ Fold04: df=50 
+ Fold01: df=50 
+ Fold06: df=50 
+ Fold07: df=50+ Fold09: df=50 
+ Fold10: df=50 
+ Fold08: df=50 
 
- Fold04: df=50 
- Fold01: df=50 
- Fold08: df=50 
- Fold03: df=50 
- Fold07: df=50 
- Fold05: df=50 
- Fold09: df=50 
- Fold02: df=50 
- Fold10: df=50 
- Fold06: df=50 
Aggregating results
Fitting final model on full training set
[1] "saving model: cache/gamSpline_50"
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
+ Fold01: df=100 
+ Fold04: df=100 + Fold02: df=100 
+ Fold05: df=100 
+ Fold03: df=100 

+ Fold06: df=100 
+ Fold07: df=100 
+ Fold08: df=100 
+ Fold09: df=100 
+ Fold10: df=100 
- Fold04: df=100 
- Fold10: df=100 
- Fold03: df=100 
- Fold02: df=100 
- Fold08: df=100 
- Fold07: df=100 
- Fold09: df=100 
- Fold01: df=100 
- Fold05: df=100 
- Fold06: df=100 
Aggregating results
Fitting final model on full training set
[1] "saving model: cache/gamSpline_100"
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
+ Fold01: span=0.5, degree=1+ Fold02: span=0.5, degree=1 
 
+ Fold04: span=0.5, degree=1 
+ Fold05: span=0.5, degree=1 
+ Fold06: span=0.5, degree=1 + Fold07: span=0.5, degree=1 
+ Fold08: span=0.5, degree=1 
+ Fold09: span=0.5, degree=1 + Fold03: span=0.5, degree=1 
+ Fold10: span=0.5, degree=1 


- Fold10: span=0.5, degree=1 
- Fold03: span=0.5, degree=1 
- Fold08: span=0.5, degree=1 
- Fold02: span=0.5, degree=1 
- Fold07: span=0.5, degree=1 
- Fold09: span=0.5, degree=1 
- Fold05: span=0.5, degree=1 
- Fold01: span=0.5, degree=1 
- Fold04: span=0.5, degree=1 
- Fold06: span=0.5, degree=1 
Aggregating results
Fitting final model on full training set
[1] "saving model: cache/gamLoess_0.5_1"
Loading required package: xgboost
Loading required package: plyr
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
+ Fold01: nrounds=500, max_depth=5, eta=0.01+ Fold02: nrounds=500, max_depth=5, eta=0.01 
+ Fold03: nrounds=500, max_depth=5, eta=0.01 
 
+ Fold04: nrounds=500, max_depth=5, eta=0.01+ Fold05: nrounds=500, max_depth=5, eta=0.01 
+ Fold06: nrounds=500, max_depth=5, eta=0.01 
 + Fold08: nrounds=500, max_depth=5, eta=0.01 
+ Fold07: nrounds=500, max_depth=5, eta=0.01 
+ Fold09: nrounds=500, max_depth=5, eta=0.01 
+ Fold10: nrounds=500, max_depth=5, eta=0.01 

- Fold05: nrounds=500, max_depth=5, eta=0.01 
- Fold03: nrounds=500, max_depth=5, eta=0.01 
- Fold08: nrounds=500, max_depth=5, eta=0.01 
- Fold07: nrounds=500, max_depth=5, eta=0.01 
- Fold04: nrounds=500, max_depth=5, eta=0.01 
- Fold01: nrounds=500, max_depth=5, eta=0.01 
- Fold09: nrounds=500, max_depth=5, eta=0.01 
- Fold02: nrounds=500, max_depth=5, eta=0.01 
- Fold10: nrounds=500, max_depth=5, eta=0.01 
- Fold06: nrounds=500, max_depth=5, eta=0.01 
Aggregating results
Fitting final model on full training set
[1] "saving model: cache/xgbTree_500_5_0.01"
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
+ Fold01: nrounds=1000, max_depth=10, eta=0.001 
+ Fold04: nrounds=1000, max_depth=10, eta=0.001 + Fold02: nrounds=1000, max_depth=10, eta=0.001 
+ Fold05: nrounds=1000, max_depth=10, eta=0.001 
+ Fold06: nrounds=1000, max_depth=10, eta=0.001 
+ Fold03: nrounds=1000, max_depth=10, eta=0.001
+ Fold09: nrounds=1000, max_depth=10, eta=0.001 
+ Fold07: nrounds=1000, max_depth=10, eta=0.001 
+ Fold10: nrounds=1000, max_depth=10, eta=0.001+ Fold08: nrounds=1000, max_depth=10, eta=0.001 
  

- Fold10: nrounds=1000, max_depth=10, eta=0.001 
- Fold03: nrounds=1000, max_depth=10, eta=0.001 
- Fold05: nrounds=1000, max_depth=10, eta=0.001 
- Fold02: nrounds=1000, max_depth=10, eta=0.001 
- Fold09: nrounds=1000, max_depth=10, eta=0.001 
- Fold01: nrounds=1000, max_depth=10, eta=0.001 
- Fold07: nrounds=1000, max_depth=10, eta=0.001 
- Fold08: nrounds=1000, max_depth=10, eta=0.001 
- Fold06: nrounds=1000, max_depth=10, eta=0.001 
- Fold04: nrounds=1000, max_depth=10, eta=0.001 
Aggregating results
Fitting final model on full training set
[1] "saving model: cache/xgbTree_1000_10_0.001"
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
+ Fold01: nrounds=1000, max_depth=10, eta=0.005+ Fold02: nrounds=1000, max_depth=10, eta=0.005 
+ Fold04: nrounds=1000, max_depth=10, eta=0.005 
+ Fold07: nrounds=1000, max_depth=10, eta=0.005 
+ Fold03: nrounds=1000, max_depth=10, eta=0.005 
+ Fold09: nrounds=1000, max_depth=10, eta=0.005 
 
+ Fold05: nrounds=1000, max_depth=10, eta=0.005 
+ Fold08: nrounds=1000, max_depth=10, eta=0.005 
+ Fold06: nrounds=1000, max_depth=10, eta=0.005 
+ Fold10: nrounds=1000, max_depth=10, eta=0.005 
- Fold10: nrounds=1000, max_depth=10, eta=0.005 
- Fold08: nrounds=1000, max_depth=10, eta=0.005 
- Fold07: nrounds=1000, max_depth=10, eta=0.005 
- Fold09: nrounds=1000, max_depth=10, eta=0.005 
- Fold02: nrounds=1000, max_depth=10, eta=0.005 
- Fold05: nrounds=1000, max_depth=10, eta=0.005 
- Fold06: nrounds=1000, max_depth=10, eta=0.005 
- Fold04: nrounds=1000, max_depth=10, eta=0.005 
- Fold03: nrounds=1000, max_depth=10, eta=0.005 
- Fold01: nrounds=1000, max_depth=10, eta=0.005 
Aggregating results
Fitting final model on full training set
[1] "saving model: cache/xgbTree_1000_10_0.005"
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
loaded caret and set parent environment
+ Fold01: nrounds=1000, max_depth=15, eta=0.005 
+ Fold02: nrounds=1000, max_depth=15, eta=0.005 
+ Fold03: nrounds=1000, max_depth=15, eta=0.005 
+ Fold08: nrounds=1000, max_depth=15, eta=0.005 
+ Fold04: nrounds=1000, max_depth=15, eta=0.005 
+ Fold07: nrounds=1000, max_depth=15, eta=0.005 
+ Fold09: nrounds=1000, max_depth=15, eta=0.005 
+ Fold06: nrounds=1000, max_depth=15, eta=0.005 
+ Fold05: nrounds=1000, max_depth=15, eta=0.005 
+ Fold10: nrounds=1000, max_depth=15, eta=0.005 
- Fold01: nrounds=1000, max_depth=15, eta=0.005 
- Fold08: nrounds=1000, max_depth=15, eta=0.005 
- Fold06: nrounds=1000, max_depth=15, eta=0.005 
- Fold03: nrounds=1000, max_depth=15, eta=0.005 
- Fold07: nrounds=1000, max_depth=15, eta=0.005 
- Fold05: nrounds=1000, max_depth=15, eta=0.005 
- Fold10: nrounds=1000, max_depth=15, eta=0.005 
- Fold04: nrounds=1000, max_depth=15, eta=0.005 
- Fold09: nrounds=1000, max_depth=15, eta=0.005 
- Fold02: nrounds=1000, max_depth=15, eta=0.005 
Aggregating results
Fitting final model on full training set
[1] "saving model: cache/xgbTree_1000_15_0.005"
> 
> stopCluster(cl)
> 
> 
> xyplot(resamples(model_list))
Warning messages:
Warning messages:
Warning messages:
Warning messages:
1: In predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type ==  :
  prediction from a rank-deficient fit may be misleading
Warning messages:
2: In predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type ==  :
  prediction from a rank-deficient fit may be misleading
1: In predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type ==  :
  prediction from a rank-deficient fit may be misleading
Warning messages:
2: In predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type ==  :
  prediction from a rank-deficient fit may be misleading
Warning messages:
Warning messages:
Warning messages:
Warning messages:
1: In predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type ==  :
  prediction from a rank-deficient fit may be misleading
1: In predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type ==  :
  prediction from a rank-deficient fit may be misleading
2: In predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type ==  :
  prediction from a rank-deficient fit may be misleading
1: In predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type ==  :
  prediction from a rank-deficient fit may be misleading
3: In predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type ==  :
  prediction from a rank-deficient fit may be misleading
2: In predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type ==  :
  prediction from a rank-deficient fit may be misleading
3: In predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type ==  :
  prediction from a rank-deficient fit may be misleading
3: In predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type ==  :
  prediction from a rank-deficient fit may be misleading
4: In predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type ==  :
  prediction from a rank-deficient fit may be misleading
4: In predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type ==  :
  prediction from a rank-deficient fit may be misleading
1: In predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type ==  :
  prediction from a rank-deficient fit may be misleading
3: In predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type ==  :
  prediction from a rank-deficient fit may be misleading
1: In predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type ==  :
  prediction from a rank-deficient fit may be misleading
4: In predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type ==  :
  prediction from a rank-deficient fit may be misleading
1: In predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type ==  :
  prediction from a rank-deficient fit may be misleading
1: In predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type ==  :
  prediction from a rank-deficient fit may be misleading
1: In predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type ==  :
  prediction from a rank-deficient fit may be misleading
2: In predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type ==  :
  prediction from a rank-deficient fit may be misleading
2: In predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type ==  :
  prediction from a rank-deficient fit may be misleading
3: In predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type ==  :
  prediction from a rank-deficient fit may be misleading
3: In predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type ==  :
  prediction from a rank-deficient fit may be misleading
4: In predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type ==  :
  prediction from a rank-deficient fit may be misleading
4: In predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type ==  :
  prediction from a rank-deficient fit may be misleading
4: In predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type ==  :
  prediction from a rank-deficient fit may be misleading
2: In predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type ==  :
  prediction from a rank-deficient fit may be misleading
2: In predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type ==  :
  prediction from a rank-deficient fit may be misleading
2: In predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type ==  :
  prediction from a rank-deficient fit may be misleading
3: In predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type ==  :
  prediction from a rank-deficient fit may be misleading
2: In predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type ==  :
  prediction from a rank-deficient fit may be misleading
3: In predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type ==  :
  prediction from a rank-deficient fit may be misleading
3: In predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type ==  :
  prediction from a rank-deficient fit may be misleading
4: In predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type ==  :
  prediction from a rank-deficient fit may be misleading
3: In predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type ==  :
  prediction from a rank-deficient fit may be misleading
4: In predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type ==  :
  prediction from a rank-deficient fit may be misleading
4: In predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type ==  :
  prediction from a rank-deficient fit may be misleading
4: In predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type ==  :
  prediction from a rank-deficient fit may be misleading
> modelCor(resamples(model_list))
                  rf31 gamSpline31 gamSpline50 gamSpline100 gamLoees_1
rf31         1.0000000   0.7829153   0.7836204    0.7858790  0.7781007
gamSpline31  0.7829153   1.0000000   0.9990988    0.9981159  0.9899441
gamSpline50  0.7836204   0.9990988   1.0000000    0.9992859  0.9898027
gamSpline100 0.7858790   0.9981159   0.9992859    1.0000000  0.9906505
gamLoees_1   0.7781007   0.9899441   0.9898027    0.9906505  1.0000000
xgbTree      0.9510864   0.8802742   0.8804081    0.8828184  0.8681542
xgbTree.1    0.8154226   0.8371721   0.8326810    0.8376176  0.8127326
xgbTree.2    0.8611540   0.8691483   0.8692351    0.8721160  0.8497947
xgbTree.3    0.8464228   0.8411397   0.8494786    0.8616873  0.8304696
               xgbTree xgbTree.1 xgbTree.2 xgbTree.3
rf31         0.9510864 0.8154226 0.8611540 0.8464228
gamSpline31  0.8802742 0.8371721 0.8691483 0.8411397
gamSpline50  0.8804081 0.8326810 0.8692351 0.8494786
gamSpline100 0.8828184 0.8376176 0.8721160 0.8616873
gamLoees_1   0.8681542 0.8127326 0.8497947 0.8304696
xgbTree      1.0000000 0.9376433 0.9608334 0.9213366
xgbTree.1    0.9376433 1.0000000 0.9688349 0.8984000
xgbTree.2    0.9608334 0.9688349 1.0000000 0.9357196
xgbTree.3    0.9213366 0.8984000 0.9357196 1.0000000
> 
> greedy_ensemble <- caretEnsemble(model_list, optFUN=greedOptGini)
> summary(greedy_ensemble)
The following models were ensembled: rf31, gamSpline100, xgbTree, xgbTree.2, xgbTree.3 
They were weighted: 
0.07 0.09 0.46 0.31 0.07
The resulting RMSE is: 3.7852
The fit for each individual model on the RMSE is: 
       method   metric  metricSD
         rf31 3.824924 0.1098254
 gamSpline100 3.863440 0.1203940
      xgbTree 3.793438 0.1096350
    xgbTree.2 3.817156 0.1072673
    xgbTree.3 3.903131 0.0943993
> evalEnsemble(greedy_ensemble, NormalizedGini)
[1] 0.3812302
       rf gamSpline   xgbTree xgbTree.1 xgbTree.2 
0.3466169 0.3258668 0.3737379 0.3695134 0.3363035 
> test_pred <-predict(greedy_ensemble,newdata=x_test)
Warning message:
In predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type ==  :
  prediction from a rank-deficient fit may be misleading
> result <- data.frame(Id=test_data$Id, Hazard=test_pred)
> write.csv(result, file.path('output','submission.csv'), row.names=FALSE, quote=FALSE)
> 
> 
